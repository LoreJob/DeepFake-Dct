{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoreJob/DeepFake-Dct/blob/main/CNN+LSTM for Deep_Fake_Det.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Backup code\n",
        "Codice iniziale, successivamente usato in altri notebook. Questa copia serve da cuscinetto nel caso in cui dovesse rompersi qualcosa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knObUu4w4crU"
      },
      "source": [
        "## CNN+LSTM for DeepFake Detection in videos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "### Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Ram/Disk on the upper-left part of the colab â†’ Additional Connection Options\n",
        "- select GPU T4 from the Hardware Accelerator drop-down (You have just 1 hour of using)\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXnDmXR7RDr2",
        "outputId": "d0476431-97e6-4bee-c5ef-e60f622f76e8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3fE7KmKRDsH"
      },
      "source": [
        "### Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y04m-jvKRDsJ",
        "outputId": "8cfcaa14-fbc5-49b5-aa40-224c6d87aeb5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpFCFcx5d8qb",
        "outputId": "181a822f-7797-496a-cf09-f2e6bc72f429"
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEf1Ql815LY7"
      },
      "source": [
        "## Packages\n",
        "As the title says, we are using the tensorflow package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GGdFw_avd6k8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hr2gRHq5PTU"
      },
      "source": [
        "### Importing the dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "HEIGHT, WIDTH = 64, 64\n",
        "N_FRAMES = 20  # Number of frames of a video that will be fed to the model as one sequence\n",
        "DATASET_DIR = \"Video Dataset Small\"\n",
        "CLASSES = [\"fake\", \"real\"]\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second approach \n",
        "Introducing tools in order to get a lighter verion of the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importing the dataset using data generators\n",
        "\n",
        "This method is not loading all the data, but is creating some data generators that are extracting data when it's needed, with a batch for batch approach.\n",
        "This methos is light on the RAM but slow when you run the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating the frame extractor \n",
        "##### First approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def frames_extraction(video_path):\n",
        "    frames_list = []\n",
        "\n",
        "    #Reading the video\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "\n",
        "    #Getting total number of frames\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    #Selecting only the wanted amount of frames\n",
        "    skip_frames_window = max(int(video_frames_count/N_FRAMES), 1)\n",
        "\n",
        "    #Looping to the selected frames\n",
        "    for frame_counter in range(N_FRAMES):\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "        \n",
        "        #reading the frames\n",
        "        success, frame = video_reader.read()\n",
        "        #breaking if the read is not successfull\n",
        "        if not success:\n",
        "            break\n",
        "        \n",
        "        #setting the shape and normalizing the frame (each frame should have a value between 0-1)\n",
        "        resized_frame = cv2.resize(frame, (HEIGHT, WIDTH))\n",
        "        normalized_frame = resized_frame / 255\n",
        "\n",
        "        frames_list.append(normalized_frame)\n",
        "\n",
        "    video_reader.release()\n",
        "    return frames_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### second approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def frames_extraction(video_path):\n",
        "#     frames_list = []\n",
        "#     video_reader = cv2.VideoCapture(video_path)\n",
        "#     video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "#     skip_frames_window = max(int(video_frames_count / N_FRAMES), 1)\n",
        "\n",
        "#     for frame_counter in range(N_FRAMES):\n",
        "#         video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "#         success, frame = video_reader.read()\n",
        "#         if not success:\n",
        "#             break\n",
        "#         resized_frame = cv2.resize(frame, (HEIGHT, WIDTH))\n",
        "#         normalized_frame = resized_frame / 255.0\n",
        "#         frames_list.append(normalized_frame)\n",
        "\n",
        "#     video_reader.release()\n",
        "#     return frames_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating the dataset creator\n",
        "\n",
        "##### First approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading videos from folder\n",
        "def create_dataset(path = DATASET_DIR, classes = CLASSES, n_frames = N_FRAMES, split = None):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for class_idx, class_name in enumerate(classes):\n",
        "        print(f'Extracting data of: {split}\\\\{class_name}')\n",
        "\n",
        "        file_list = os.listdir(os.path.join(path, split, class_name))\n",
        "\n",
        "        for file_name in file_list:\n",
        "            video_file_path = os.path.join(path, split, class_name, file_name)\n",
        "            frames = frames_extraction(video_file_path)\n",
        "\n",
        "            if len(frames) == n_frames:\n",
        "\n",
        "                # Append the data to their repective lists.\n",
        "                features.append(frames)\n",
        "                labels.append(class_idx)\n",
        "\n",
        "    features = np.asarray(features)\n",
        "    labels = np.array(labels)  \n",
        "\n",
        "    return features, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Second approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class VideoDataGenerator(tf.keras.utils.Sequence):\n",
        "#     def __init__(self, dataset_dir, split, classes, n_frames, batch_size):\n",
        "#         self.dataset_dir = dataset_dir\n",
        "#         self.split = split\n",
        "#         self.classes = classes\n",
        "#         self.n_frames = n_frames\n",
        "#         self.batch_size = batch_size\n",
        "#         self.file_paths = []\n",
        "#         self.labels = []\n",
        "#         self._load_file_paths_and_labels()\n",
        "\n",
        "#     def _load_file_paths_and_labels(self):\n",
        "#         for class_idx, class_name in enumerate(self.classes):\n",
        "#             class_dir = os.path.join(self.dataset_dir, self.split, class_name)\n",
        "#             for file_name in os.listdir(class_dir):\n",
        "#                 self.file_paths.append(os.path.join(class_dir, file_name))\n",
        "#                 self.labels.append(class_idx)\n",
        "#         self.labels = tf.keras.utils.to_categorical(self.labels, num_classes=len(self.classes))\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.file_paths) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         batch_x = self.file_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "#         batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "#         return np.array([frames_extraction(file_path) for file_path in batch_x]), np.array(batch_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading Datasets\n",
        "\n",
        "##### First approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train, validation, and test datasets\n",
        "X_train, y_train = create_dataset(split = \"Train\")\n",
        "X_val, y_val = create_dataset(split='Val')\n",
        "X_test, y_test = create_dataset(split='Test')\n",
        "\n",
        "# Normalize y labels to be categorical\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(CLASSES))\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=len(CLASSES))\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=len(CLASSES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).shuffle(len(X_val))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "# Batch datasets\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Prefetch for Efficiency\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Second approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create generators for train, validation, and test datasets\n",
        "# train_generator = VideoDataGenerator(DATASET_DIR, 'Train', CLASSES, N_FRAMES, BATCH_SIZE)\n",
        "# val_generator = VideoDataGenerator(DATASET_DIR, 'Val', CLASSES, N_FRAMES, BATCH_SIZE)\n",
        "# test_generator = VideoDataGenerator(DATASET_DIR, 'Test', CLASSES, N_FRAMES, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading datasets on the ram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def frames_extraction(video_path):\n",
        "    frames_list = []\n",
        "    video_reader = cv2.VideoCapture(video_path)\n",
        "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    skip_frames_window = max(int(video_frames_count / N_FRAMES), 1)\n",
        "\n",
        "    for frame_counter in range(N_FRAMES):\n",
        "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
        "        success, frame = video_reader.read()\n",
        "        if not success:\n",
        "            break\n",
        "        resized_frame = cv2.resize(frame, (HEIGHT, WIDTH))\n",
        "        normalized_frame = resized_frame / 255.0\n",
        "        frames_list.append(normalized_frame)\n",
        "\n",
        "    video_reader.release()\n",
        "    return frames_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataset(path=DATASET_DIR, classes=CLASSES, n_frames=N_FRAMES, split=None):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for class_idx, class_name in enumerate(classes):\n",
        "        print(f'Extracting data of: {split}\\\\{class_name}')\n",
        "        class_dir = os.path.join(path, split, class_name)\n",
        "        file_list = os.listdir(class_dir)\n",
        "\n",
        "        for file_name in file_list:\n",
        "            video_file_path = os.path.join(class_dir, file_name)\n",
        "            frames = frames_extraction(video_file_path)\n",
        "            if len(frames) == n_frames:\n",
        "                features.append(frames)\n",
        "                labels.append(class_idx)\n",
        "\n",
        "    features = np.asarray(features, dtype=np.float16)\n",
        "    labels = np.array(labels)\n",
        "    return features, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving preprocessed datasets\n",
        "\n",
        "To speed up things i saved the files in order to access the datasets already preprocessed. \n",
        "\n",
        "The running time for processing datasets is quite long and depend on the machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_dir = \"Video Dataset Small/Processed_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, y_train = create_dataset(split=\"Train\")\n",
        "# y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(CLASSES))\n",
        "\n",
        "# np.save(os.path.join(save_dir, 'X_train.npy'), X_train)\n",
        "# np.save(os.path.join(save_dir, 'y_train.npy'), y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_test, y_test = create_dataset(split=\"Test\")\n",
        "# y_test = tf.keras.utils.to_categorical(y_test, num_classes=len(CLASSES))\n",
        "\n",
        "# np.save(os.path.join(save_dir, 'X_test.npy'), X_test)\n",
        "# np.save(os.path.join(save_dir, 'y_test.npy'), y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_val, y_val = create_dataset(split=\"Val\")\n",
        "# y_val = tf.keras.utils.to_categorical(y_val, num_classes=len(CLASSES))\n",
        "\n",
        "# np.save(os.path.join(save_dir, 'X_val.npy'), X_val)\n",
        "# np.save(os.path.join(save_dir, 'y_val.npy'), y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = np.load(os.path.join(save_dir, 'X_train.npy'))\n",
        "y_train = np.load(os.path.join(save_dir, 'y_train.npy'))\n",
        "X_val = np.load(os.path.join(save_dir, 'X_val.npy'))\n",
        "y_val = np.load(os.path.join(save_dir, 'y_val.npy'))\n",
        "X_test = np.load(os.path.join(save_dir, 'X_test.npy'))\n",
        "y_test = np.load(os.path.join(save_dir, 'y_test.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model\n",
        "### Building the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here we will add the model that performed the best on the img dataset without the last dense layer\n",
        "\n",
        "CNN_model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.1),  # Dropout layer with 10% rate\n",
        "\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.1),  # Dropout layer with 10% rate\n",
        "\n",
        "    layers.Conv2D(64, (5, 5), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.1),  # Dropout layer with 10% rate\n",
        "\n",
        "    layers.Flatten(),\n",
        "\n",
        "    layers.Dense(256, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating the full model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CNN_LSTM_model = models.Sequential()\n",
        "# Input\n",
        "CNN_LSTM_model.add(layers.Input((N_FRAMES, HEIGHT, WIDTH, 3))) # 3 are the channels\n",
        "# Adding the time distributed CNN\n",
        "CNN_LSTM_model.add(layers.TimeDistributed(CNN_model)) \n",
        "# Creating the LSTM part\n",
        "CNN_LSTM_model.add(layers.LSTM(64, return_sequences=False))\n",
        "CNN_LSTM_model.add(layers.Dense(64, activation='relu'))\n",
        "CNN_LSTM_model.add(layers.Dropout(0.25)) # Dropout layer with 25% rate\n",
        "CNN_LSTM_model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "CNN_LSTM_model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "CNN_LSTM_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='model_epoch_{epoch:02d}.keras',\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch')\n",
        "\n",
        "history = CNN_LSTM_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=20,\n",
        "    callbacks=[checkpoint_callback])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
