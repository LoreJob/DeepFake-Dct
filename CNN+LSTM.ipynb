{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoreJob/DeepFake-Dct/blob/main/CNN+LSTM Third approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knObUu4w4crU"
      },
      "source": [
        "# CNN+LSTM for DeepFake Detection in videos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Ram/Disk on the upper-left part of the colab → Additional Connection Options\n",
        "- select GPU T4 from the Hardware Accelerator drop-down (You have just 1 hour of using)\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXnDmXR7RDr2",
        "outputId": "d0476431-97e6-4bee-c5ef-e60f622f76e8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3fE7KmKRDsH"
      },
      "source": [
        "## Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y04m-jvKRDsJ",
        "outputId": "8cfcaa14-fbc5-49b5-aa40-224c6d87aeb5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpFCFcx5d8qb",
        "outputId": "181a822f-7797-496a-cf09-f2e6bc72f429"
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEf1Ql815LY7"
      },
      "source": [
        "## Packages\n",
        "As the title says, we are using the tensorflow package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GGdFw_avd6k8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from pathos.multiprocessing import ProcessingPool as Pool\n",
        "from multiprocessing import cpu_count\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "VIDEO_WIDTH, VIDEO_HEIGHT = 64, 64\n",
        "MAX_FRAMES = 20  # Number of frames of a video that will be fed to the model as one sequence\n",
        "DATASET_DIR = \"Video Dataset Small\"\n",
        "PROCESSED_DIR = \"Video Dataset Small/Processed_data\"\n",
        "CLASSES = [\"fake\", \"real\"]\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hr2gRHq5PTU"
      },
      "source": [
        "## Importing the dataset using data generators\n",
        "\n",
        "This method is not loading all the data, but is creating some data generators that are extracting data when it's needed, with a batch for batch approach.\n",
        "This methos is light on the RAM but slow when you run the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To speed up the model, we preprocessed data, in order to speed up training by reducing the time spent on loading and processing videos during each epoch.\n",
        "\n",
        "YOU DON'T NEED TO RUN THIS CELL unless your \"Video Dataset Small/Processed_data\" folder is empty or you want to change Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Train real: 100%|██████████| 7000/7000 [34:23<00:00,  3.39it/s]\n",
            "Processing Train fake: 100%|██████████| 7000/7000 [22:25<00:00,  5.20it/s] \n",
            "Processing Val real: 100%|██████████| 1500/1500 [08:48<00:00,  2.84it/s]\n",
            "Processing Val fake: 100%|██████████| 1500/1500 [04:40<00:00,  5.35it/s]\n",
            "Processing Test real: 100%|██████████| 1500/1500 [07:41<00:00,  3.25it/s]\n",
            "Processing Test fake: 100%|██████████| 1500/1500 [03:41<00:00,  6.77it/s]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    if total_frames == 0:\n",
        "        return np.zeros((MAX_FRAMES, VIDEO_HEIGHT, VIDEO_WIDTH, 3))\n",
        "\n",
        "    frames_to_sample = min(MAX_FRAMES, total_frames)\n",
        "    frame_indices = np.linspace(0, total_frames - 1, frames_to_sample, dtype=int)\n",
        "    \n",
        "    frames = []\n",
        "    for frame_index in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame = cv2.resize(frame, (VIDEO_WIDTH, VIDEO_HEIGHT))\n",
        "            frame = frame / 255.0  # Normalize pixel values\n",
        "            frames.append(frame)\n",
        "        else:\n",
        "            frames.append(np.zeros((VIDEO_HEIGHT, VIDEO_WIDTH, 3)))\n",
        "    \n",
        "    cap.release()\n",
        "\n",
        "    # Pad with zeros if we couldn't extract enough frames\n",
        "    if len(frames) < MAX_FRAMES:\n",
        "        padding = [np.zeros((VIDEO_HEIGHT, VIDEO_WIDTH, 3)) for _ in range(MAX_FRAMES - len(frames))]\n",
        "        frames.extend(padding)\n",
        "\n",
        "    return np.array(frames[:MAX_FRAMES])\n",
        "\n",
        "def process_dataset():\n",
        "    for subset in ['Train', 'Val', 'Test']:\n",
        "        for class_name in ['real', 'fake']:\n",
        "            input_dir = os.path.join(DATASET_DIR, subset, class_name)\n",
        "            output_dir = os.path.join(PROCESSED_DIR, subset, class_name)\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            \n",
        "            video_files = [f for f in os.listdir(input_dir) if f.endswith('.mp4')]\n",
        "            \n",
        "            for video_file in tqdm(video_files, desc=f\"Processing {subset} {class_name}\"):\n",
        "                video_path = os.path.join(input_dir, video_file)\n",
        "                processed_frames = preprocess_video(video_path)\n",
        "                \n",
        "                output_path = os.path.join(output_dir, video_file.replace('.mp4', '.npy'))\n",
        "                np.save(output_path, processed_frames)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running time for the previous cell on my pc was one hour. Colab is slower -Manu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoDataGenerator(Sequence):\n",
        "    def __init__(self, data_dir, subset, batch_size=BATCH_SIZE):\n",
        "        self.data_dir = data_dir\n",
        "        self.subset = subset\n",
        "        self.batch_size = batch_size\n",
        "        self.classes = ['real', 'fake']\n",
        "        self.videos = self._get_video_paths()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def _get_video_paths(self):\n",
        "        videos = []\n",
        "        subset_dir = os.path.join(self.data_dir, self.subset)\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(subset_dir, class_name)\n",
        "            for video_name in os.listdir(class_dir):\n",
        "                if video_name.endswith('.npy'):\n",
        "                    videos.append((os.path.join(class_dir, video_name), self.classes.index(class_name)))\n",
        "        return videos\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_videos = self.videos[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_frames = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for video_path, label in batch_videos:\n",
        "            frames = np.load(video_path)\n",
        "            batch_frames.append(frames)\n",
        "            batch_labels.append(label)\n",
        "\n",
        "        return np.array(batch_frames), np.array(batch_labels)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_generator = VideoDataGenerator(PROCESSED_DIR, \"Train\")\n",
        "val_generator = VideoDataGenerator(PROCESSED_DIR, \"Val\")\n",
        "test_generator = VideoDataGenerator(PROCESSED_DIR, \"Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here we will add the model that performed the best on the img dataset without the last dense layer\n",
        "\n",
        "CNN_model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.1),  # Dropout layer with 10% rate\n",
        "\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.1),  # Dropout layer with 10% rate\n",
        "\n",
        "    layers.Conv2D(64, (5, 5), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.1),  # Dropout layer with 10% rate\n",
        "\n",
        "    layers.Flatten(),\n",
        "\n",
        "    layers.Dense(256, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating the full model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">466,208</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,176</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m466,208\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m82,176\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">552,609</span> (2.11 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m552,609\u001b[0m (2.11 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">552,609</span> (2.11 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m552,609\u001b[0m (2.11 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "CNN_LSTM_model = models.Sequential()\n",
        "# Input\n",
        "CNN_LSTM_model.add(layers.Input((MAX_FRAMES, VIDEO_WIDTH, VIDEO_HEIGHT, 3))) # 3 are the channels\n",
        "# Adding the time distributed CNN\n",
        "CNN_LSTM_model.add(layers.TimeDistributed(CNN_model)) \n",
        "# Creating the LSTM part\n",
        "CNN_LSTM_model.add(layers.LSTM(64, return_sequences=False))\n",
        "CNN_LSTM_model.add(layers.Dense(64, activation='relu'))\n",
        "CNN_LSTM_model.add(layers.Dropout(0.25)) # Dropout layer with 25% rate\n",
        "CNN_LSTM_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compiling the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "CNN_LSTM_model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "CNN_LSTM_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eleonoramarcassa/Desktop/UniVe/I_anno /Data Analytics and Artificial Intelligence/DeepFake-Dct/.venv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 824ms/step - accuracy: 0.5331 - loss: 0.8139 - val_accuracy: 0.6324 - val_loss: 0.6654\n",
            "Epoch 2/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 1s/step - accuracy: 0.6929 - loss: 0.5916 - val_accuracy: 0.8706 - val_loss: 0.3399\n",
            "Epoch 3/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 1s/step - accuracy: 0.8734 - loss: 0.3419 - val_accuracy: 0.9399 - val_loss: 0.2095\n",
            "Epoch 4/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 862ms/step - accuracy: 0.9313 - loss: 0.2099 - val_accuracy: 0.9593 - val_loss: 0.1485\n",
            "Epoch 5/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 876ms/step - accuracy: 0.9545 - loss: 0.1597 - val_accuracy: 0.9711 - val_loss: 0.1148\n",
            "Epoch 6/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 875ms/step - accuracy: 0.9651 - loss: 0.1331 - val_accuracy: 0.9624 - val_loss: 0.1447\n",
            "Epoch 7/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 897ms/step - accuracy: 0.9735 - loss: 0.1076 - val_accuracy: 0.9698 - val_loss: 0.1257\n",
            "Epoch 8/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 880ms/step - accuracy: 0.9749 - loss: 0.1062 - val_accuracy: 0.9748 - val_loss: 0.1071\n",
            "Epoch 9/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 879ms/step - accuracy: 0.9808 - loss: 0.0891 - val_accuracy: 0.9805 - val_loss: 0.0921\n",
            "Epoch 10/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 911ms/step - accuracy: 0.9822 - loss: 0.0860 - val_accuracy: 0.9815 - val_loss: 0.0933\n",
            "Epoch 11/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 871ms/step - accuracy: 0.9827 - loss: 0.0854 - val_accuracy: 0.9782 - val_loss: 0.0983\n",
            "Epoch 12/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 886ms/step - accuracy: 0.9838 - loss: 0.0802 - val_accuracy: 0.9852 - val_loss: 0.0868\n",
            "Epoch 13/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 862ms/step - accuracy: 0.9869 - loss: 0.0716 - val_accuracy: 0.9775 - val_loss: 0.1056\n",
            "Epoch 14/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 875ms/step - accuracy: 0.9856 - loss: 0.0730 - val_accuracy: 0.9765 - val_loss: 0.1076\n",
            "Epoch 15/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 869ms/step - accuracy: 0.9844 - loss: 0.0783 - val_accuracy: 0.9839 - val_loss: 0.0880\n",
            "Epoch 16/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 874ms/step - accuracy: 0.9896 - loss: 0.0649 - val_accuracy: 0.9832 - val_loss: 0.0895\n",
            "Epoch 17/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 887ms/step - accuracy: 0.9909 - loss: 0.0607 - val_accuracy: 0.9802 - val_loss: 0.1059\n",
            "Epoch 18/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 885ms/step - accuracy: 0.9918 - loss: 0.0634 - val_accuracy: 0.9792 - val_loss: 0.1088\n",
            "Epoch 19/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 890ms/step - accuracy: 0.9908 - loss: 0.0630 - val_accuracy: 0.9842 - val_loss: 0.0850\n",
            "Epoch 20/20\n",
            "\u001b[1m437/437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 892ms/step - accuracy: 0.9884 - loss: 0.0662 - val_accuracy: 0.9808 - val_loss: 0.1013\n"
          ]
        }
      ],
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='model_epoch_{epoch:02d}.keras',\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch')\n",
        "\n",
        "history = CNN_LSTM_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,\n",
        "    callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eleonoramarcassa/Desktop/UniVe/I_anno /Data Analytics and Artificial Intelligence/DeepFake-Dct/.venv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 280ms/step - accuracy: 0.9756 - loss: 0.1018\n",
            "Test Loss: 0.09757249057292938\n",
            "Test Accuracy: 0.9784946441650391\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = CNN_LSTM_model.evaluate(test_generator, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
